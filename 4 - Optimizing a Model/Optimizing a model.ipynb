{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias/Variance Tradeoff\n",
    "\n",
    "Bias is the algorithm's tendency to consistently learn the wrong thing \n",
    "by not taking into account all the information in the data \n",
    "\n",
    "High bias is a result of the algorithm missing the relevant relations between \n",
    "features and target outputs\n",
    "\n",
    "Variance refers to an algorithm's sensetivity to small fluctuations in the traning set\n",
    "\n",
    "High variance is a result of the algorithm fitting to random noise in the traning data\n",
    "\n",
    "Low variance/High Bias\n",
    "High variance/Low Bias\n",
    "Optimum Model Complexity - low bias and low variance\n",
    "\n",
    "Total Error = (Bias + Variance) + Irreducible Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting occurs when an algorithmcannot capture the underlying trend of data\n",
    "Happens when model too simple with high bias and low variance\n",
    "\n",
    "Overfitting occurs when an algorithm fits too closely to a limited set of data\n",
    "Low bias and high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "underfit model\n",
    "high training and high test errors\n",
    "overfit model low traning error and high test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - choosing a set of optimal hyperparameters for fitting an algorithm\n",
    "\n",
    "A model parameter is a configuration variable that is internal to the model \n",
    "and whose value can be estimated from data (Parameter estimated from data)\n",
    "\n",
    "A model hyperparameter is a configaration that is external to the model, whose value cannot be estimated from data \n",
    "and whose value quides how the algorithm learns parameter values from the data\n",
    "\n",
    "Parameters from data - fare, ticket_class, etc...\n",
    "Hyperparameters for the model - max depth of the tree, features to consider, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization - Technique used to reduce overfitting by discouraging overly complex models in some way\n",
    "\n",
    "Goal of regularization - allow enough for the algorithm to learn the underlying patterns in the data but \n",
    "provide guardrails so it doesn't overfit\n",
    "\n",
    "Occam's razor - whenever possible, choose the simplest answer to a problem\n",
    "\n",
    "Examples of Regularization\n",
    "1. Ridge regression and Lasso regression - Adding a penalty to the loss function to constrain coefficients\n",
    "2. Dropout - some nodes are ignored during training which forces the other node to take on more or less responsibility for the input/output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
